# SIPB Deep Learning Group
The schedule of readings for the SIPB/Cambridge AI Deep Learning Group If you have any papers you'd like to discuss, please either make a pull request, or send an email to the group and we'll add it. Papers with implementations available are strongly preferred.


## Suggested Papers:

* [Taskonomy: Disentangling Task Transfer Learning](https://arxiv.org/pdf/1804.08328.pdf)


## Schedule:


| Date  | Paper | Implementation |
| ------------- | ------------- |------------- |
|3.21.24|[Humanoid Locomotion as Next Token Prediction](https://arxiv.org/abs/2402.19469)||
|3.14.24|[TIES-Merging: Resolving Interference When Merging Models](https://arxiv.org/abs/2306.01708)|[prateeky2806/ties-merging](https://github.com/prateeky2806/ties-merging)|
|2.8.24|[Merging Models with Fisher-Weighted Averaging](https://arxiv.org/abs/2111.09832)|[arcee-ai/mergekit](https://github.com/arcee-ai/mergekit)|
|2.1.24|[Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407)||
|1.18.24|[Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866)||
|1.04.24|[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)|[state-spaces/mamba](https://github.com/state-spaces/mamba)|
|12.07.23|[Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features)||
|11.30.23|[3D Gaussian Splatting for Real-Time Radiance Field Rendering](https://arxiv.org/abs/2308.04079)|[graphdeco-inria/gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting)|
|11.16.23|[LILO: Learning Interpretable Libraries by Compressing and Documenting Code](https://arxiv.org/abs/2310.19791)|[gabegrand/lilo](https://github.com/gabegrand/lilo)|
|11.09.23|[Human-like systematic generalization through a meta-learning neural network](https://www.nature.com/articles/s41586-023-06668-3)|[brendenlake/MLC](https://github.com/brendenlake/MLC) and [brendenlake/MLC-ML](https://github.com/brendenlake/MLC-ML)|
|9.28.23|[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)|[huggingface/transformers/examples/research_projects/rag](https://github.com/huggingface/transformers/tree/main/examples/research_projects/rag)|
|9.14.23|[Gradient-based Adversarial Attacks against Text Transformers](https://arxiv.org/abs/2104.13733)|[facebookresearch/text-adversarial-attack](https://github.com/facebookresearch/text-adversarial-attack)|
|8.10.23|[Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)|[noahshinn024/reflexion](https://github.com/noahshinn024/reflexion)|
|6.15.23|[RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)|[BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)|
|5.18.23|[Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)||
|5.11.23|[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)|[tloen/alpaca-lora](https://github.com/tloen/alpaca-lora) and [huggingface/blog/lora](https://huggingface.co/blog/lora)|
|5.04.23|[Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)|[HazyResearch/state-spaces](https://github.com/HazyResearch/state-spaces)|
|4.06.23|[Generating Sequences by Learning to Self-Correct](https://arxiv.org/abs/2211.00053)||
|3.30.23|[The Capacity for Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2302.07459)||
|3.23.23|[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)|[facebookresearch/llama](https://github.com/facebookresearch/llama) and [huggingface/llama](https://github.com/huggingface/transformers/tree/main/src/transformers/models/llama)|
|3.16.23|[Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045)||
|3.02.23|[Guiding Pretraining in Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2302.06692)||
|2.23.23|[Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)||
|2.16.23|[What learning algorithm is in-context learning? Investigations with linear models](https://openreview.net/forum?id=0g0X4H8yN4I)|[ekinakyurek/incontext](https://github.com/ekinakyurek/google-research/tree/master/incontext)|
|2.09.23|[Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation](https://arxiv.org/abs/2106.04399)|[Tutorial](https://milayb.notion.site/The-GFlowNet-Tutorial-95434ef0e2d94c24aab90e69b30be9b3)|
|1.26.23|[Mastering Diverse Domains through World Models](https://arxiv.org/abs/2301.04104)||
|1.12.23|[The Forward-Forward Algorithm: Some Preliminary Investigations](https://arxiv.org/abs/2212.13345)||
|12.08.22|[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)||
|9.22.22|[Git Re-Basin: Merging Models modulo Permutation Symmetries](https://arxiv.org/abs/2209.04836)||
|9.08.22|[Transformers are Sample-Efficient World Models](https://arxiv.org/abs/2209.00588)||
|8.25.22|[A Path Towards Autonomous Machine Intelligence](https://openreview.net/pdf?id=BZ5a1r-kVsf)||
|8.18.22|[Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)|[microsoft/mup](https://github.com/microsoft/mup)|
|7.14.22|[Learning Iterative Reasoning through Energy Minimization](https://arxiv.org/abs/2206.15448)|[yilundu/irem_code_release](https://github.com/yilundu/irem_code_release)|
|6.16.22|[Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/abs/2010.01412)|[google-research/sam](https://github.com/google-research/sam)|
|5.26.22|[Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)||
|4.28.22|[A Modern Self-Referential Weight Matrix That Learns to Modify Itself](https://arxiv.org/abs/2202.05780)|[IDSIA/modern-srwm](https://github.com/IDSIA/modern-srwm)|
|4.14.22|[Hierarchical Perceiver](https://arxiv.org/abs/2202.10890)||
|3.24.22|[Dual Diffusion Implicit Bridges for Image-to-Image Translation](https://arxiv.org/abs/2203.08382)||
|3.10.22|[Understanding Generalization through Visualizations](https://arxiv.org/abs/1906.03291)|[wronnyhuang/gen-viz](https://github.com/wronnyhuang/gen-viz)|
|2.17.22|[Divide and Contrast: Self-supervised Learning from Uncurated Data](https://arxiv.org/abs/2105.08054)||
|2.10.22|[Investigating Human Priors for Playing Video Games](https://arxiv.org/abs/1802.10217)|[rach0012/humanRL_prior_games](https://github.com/rach0012/humanRL_prior_games)|
|1.27.22|[data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555)|[pytorch/data2vec](https://github.com/pytorch/fairseq/tree/main/examples/data2vec)|
|1.20.22|[Consistent Video Depth Estimation](https://arxiv.org/abs/2004.15021)|[facebookresearch/consistent_depth](https://github.com/facebookresearch/consistent_depth)|
|1.13.22|[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)||
|12.02.21|[Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)||
|11.18.21|[(StyleGan3) Alias-Free Generative Adversarial Networks](https://arxiv.org/abs/2106.12423)|[NVlabs/stylegan3](https://github.com/NVlabs/stylegan3)|
|11.04.21|[Do Vision Transformers See Like Convolutional Neural Networks?](https://arxiv.org/abs/2108.08810)||
|10.21.21|[CoBERL: Contrastive BERT for Reinforcement Learning](https://arxiv.org/abs/2107.05431)||
|10.14.21|[WarpedGANSpace: Finding non-linear RBF paths in GAN latent space](https://arxiv.org/abs/2109.13357)|[chi0tzp/WarpedGANSpace](https://github.com/chi0tzp/WarpedGANSpace)|
|10.06.21|[RAFT: Recurrent All-Pairs Field Transforms for Optical Flow](https://arxiv.org/abs/2003.12039)|[princeton-vl/RAFT](https://github.com/princeton-vl/RAFT)|
|9.16.21|[Bootstrapped Meta-Learning](https://arxiv.org/abs/2109.04504)||
|9.09.21|[Program Synthesis with Large Language Models](https://arxiv.org/abs/2108.07732)||
|8.19.21|[Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795)|[deepmind/perceiver](https://github.com/deepmind/deepmind-research/tree/master/perceiver)|
|8.12.21|[Reward is enough](https://www.sciencedirect.com/science/article/pii/S0004370221000862)||
|8.05.21|[Learning Compositional Rules via Neural Program Synthesis](https://arxiv.org/abs/2003.05562)|[mtensor/rulesynthesis](https://github.com/mtensor/rulesynthesis)|
|6.24.21|[Thinking Like Transformers](https://arxiv.org/abs/2106.06981)||
|6.17.21|[Equilibrium Propagation: Bridging the Gap between Energy-Based Models and Backpropagation](https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full)||
|6.10.21|[Unsupervised Learning by Competing Hidden Units](https://www.pnas.org/content/pnas/116/16/7723.full.pdf)||
|5.27.21|[Pay Attention to MLPs](https://arxiv.org/abs/2105.08050)||
|5.20.21|[Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards](https://arxiv.org/abs/1907.10247)||
|5.13.21|[Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)||
|5.06.21|[Implicit Neural Representations with Periodic Activation Functions](https://arxiv.org/abs/2006.09661)|[vsitzmann/siren](https://github.com/vsitzmann/siren)|
|4.29.21|[How to represent part-whole hierarchies in a neural network](https://arxiv.org/abs/2102.12627)|[lucidrains/glom-pytorch](https://github.com/lucidrains/glom-pytorch) [RedRyan111/GLOM](https://github.com/RedRyan111/GLOM) [ArneBinder/GlomImpl](https://github.com/ArneBinder/GlomImpl)|
|4.15.21|[Perceiver: General Perception with Iterative Attention](https://arxiv.org/abs/2103.03206)||
|4.01.21|[Synthetic Returns for Long-Term Credit Assignment](https://arxiv.org/abs/2102.12425)||
|3.25.21|[The Pitfalls of Simplicity Bias in Neural Networks](https://arxiv.org/abs/2006.07710)||
|3.18.21|[Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)||
|3.11.21|[Meta Learning Backpropagation And Improving It](https://arxiv.org/abs/2012.14905)||
|3.04.21|[Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)|[CompVis/taming-transformers](https://github.com/CompVis/taming-transformers)|
|2.18.21|[Pre-training without Natural Images](https://arxiv.org/abs/2101.08515)|[hirokatsukataoka16/FractalDB-Pretrained-ResNet-PyTorch](https://github.com/hirokatsukataoka16/FractalDB-Pretrained-ResNet-PyTorch)|
|2.11.21|[Revisiting Locally Supervised Learning: an Alternative to End-to-end Training](https://arxiv.org/abs/2101.10832)|[blackfeather-wang/InfoPro-Pytorch](https://github.com/blackfeather-wang/InfoPro-Pytorch)|
|2.04.21|[Neural Power Units](https://arxiv.org/abs/2006.01681)||
|1.28.21|[Representation Learning via Invariant Causal Mechanisms](https://arxiv.org/abs/2010.07922)||
|1.21.21|[γ-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction](https://arxiv.org/abs/2010.14496)|[JannerM/gamma-models](https://github.com/JannerM/gamma-models)|
|1.14.21|[Improving Generalisation for Temporal Difference Learning: The Successor Representation](http://www.gatsby.ucl.ac.uk/~dayan/papers/d93b.pdf)||
|12.17.20|[Learning Associative Inference Using Fast Weight Memory](https://arxiv.org/abs/2011.07831)||
||Hopfield Networks cycle ends||
|12.10.20|[Hopfield Networks is All You Need](https://arxiv.org/abs/2008.02217)|[ml-jku/hopfield-layers](https://github.com/ml-jku/hopfield-layers)|
|12.03.20|[On a model of associative memory with huge storage capacity](https://arxiv.org/abs/1702.01929)||
|11.19.20|[Dense Associative Memory for Pattern Recognition](https://arxiv.org/abs/1606.01164)||
|11.12.20|[Neural Networks and Physical Systems with Emergent Collective Computational Abilities (= "the Hopfield Networks paper")](https://www.pnas.org/content/pnas/79/8/2554.full.pdf)||
||Hopfield Networks cycle of papers - from the original paper on Hopfield networks to "Hopfield Networks is All You Need"||
|11.05.20|[Training Generative Adversarial Networks with Limited Data](https://arxiv.org/abs/2006.06676)|[NVlabs/stylegan2-ada](https://github.com/NVlabs/stylegan2-ada)|
|10.29.20|[Memories from patterns: Attractor and integrator networks in the brain](https://fietelabmit.files.wordpress.com/2020/05/mikail_ctsattractor_review-may28draft-1.pdf)||
|10.15.20|[Entities as Experts: Sparse Memory Access with Entity Supervision](https://arxiv.org/abs/2004.07202)||
|10.08.20|[A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)||
|10.01.20|[It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://arxiv.org/abs/2009.07118)|[timoschick/pet](https://github.com/timoschick/pet)|
|9.24.20|[End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)|[facebookresearch/detr](https://github.com/facebookresearch/detr)|
|9.17.20|[Gated Linear Networks](https://arxiv.org/abs/1910.01526)||
|7.23.20|[A Random Matrix Perspective on Mixtures of Nonlinearities for Deep Learning](https://arxiv.org/abs/1912.00827)||
|7.02.20|[DreamCoder: Building interpretable hierarchical knowledge representations with wake-sleep Bayesian program learning](https://web.mit.edu/ellisk/www/documents/dreamcoder_with_supplement.pdf)|[ellisk42/ec](https://github.com/ellisk42/ec)|
|6.18.20|[SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver](https://arxiv.org/abs/1905.12149)|[locuslab/SATNet](https://github.com/locuslab/SATNet)|
|6.4.20|[Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)||
|5.28.20|[Complexity control by gradient descent in deep networks](https://www.nature.com/articles/s41467-020-14663-9)||
|5.21.20|[What Can Learned Intrinsic Rewards Capture?](https://arxiv.org/abs/1912.05500)||
|5.14.20|[COMET: Commonsense Transformers for Automatic Knowledge Graph Construction](https://arxiv.org/abs/1906.05317)||
|5.7.20|[Write, Execute, Assess: Program Synthesis With a REPL](http://papers.nips.cc/paper/9116-write-execute-assess-program-synthesis-with-a-repl.pdf)|[flxsosa/ProgramSearch](https://github.com/flxsosa/ProgramSearch)|
|4.23.20|[Graph Representations for Higher-Order Logic and Theorem Proving](https://arxiv.org/abs/1905.10006)||
|4.16.20|[Mathematical Reasoning in Latent Space](https://openreview.net/pdf?id=Ske31kBtPr)||
|4.9.20|[MEMO: A Deep Network for Flexible Combination of Episodic Memories](https://arxiv.org/abs/2001.10913)||
|4.2.20|[Creating High Resolution Images with a Latent Adversarial Generator](https://arxiv.org/abs/2003.02365)||
|3.26.20|[Invertible Residual Networks](https://arxiv.org/abs/1811.00995)||
|3.5.20|[Value-driven Hindsight Modelling](https://arxiv.org/abs/2002.08329)||
|2.27.20|[Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958)||
|2.13.20|[Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365)||
|2.6.20|[Automated curricula through setter-solver interactions](https://arxiv.org/abs/1909.12892)||
|1.30.20|[Protein structure prediction ...](https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25834)|[deepmind](https://github.com/deepmind/deepmind-research/tree/master/alphafold_casp13)|
|1.23.20|[Putting An End to End-to-End: Gradient-Isolated Learning of Representations](https://arxiv.org/abs/1905.11786)||
|1.16.20|[Normalizing Flows: An Introduction and Review of Current Methods](https://arxiv.org/abs/1908.09257)||
|12.19.19|[Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265)||
|12.5.19|[On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)||
|11.21.19|[Understanding the Neural Tangent Kernel](https://rajatvd.github.io/NTK/)|[rajatvd](https://github.com/rajatvd/NTK)|
|11.14.19|[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)||
|11.7.19|[Learning to Predict Without Looking Ahead: World Models Without Forward Prediction](https://arxiv.org/abs/1910.13038)||
|10.31.19|[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)||
|10.24.19|[N-BEATS: Neural basis expansion analysis for interpretable time series forecasting](https://arxiv.org/abs/1905.10437)||
|10.17.19|[Unsupervised Doodling and Painting with Improved SPIRAL](https://arxiv.org/abs/1910.01007)||
|10.10.19|[Adversarial Robustness as a Prior for Learned Representations](https://arxiv.org/abs/1906.00945)|[MadryLab](https://github.com/MadryLab/robust_representations)|
|10.3.19|[Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks](https://arxiv.org/abs/1805.12076)||
|9.26.19|[Image Transformer](https://arxiv.org/abs/1802.05751)||
|9.19.19|[Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446)||
|9.12.19|[Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)||
|9.5.19|[Neural Text Generation with Unlikelihood Training](https://arxiv.org/abs/1908.04319)||
|8.29.19|[Learning Representations by Maximizing Mutual Information Across Views](https://arxiv.org/abs/1906.00910)||
|break|switch from Tuesdays to Thursdays after the break||
|6.11.19|[BERT Rediscovers the Classical NLP Pipeline](https://arxiv.org/abs/1905.05950)||
|6.4.19|[Semantic Visual Localization](https://arxiv.org/abs/1712.05773)||
|5.28.19|[AlgoNet: C^∞ Smooth Algorithmic Neural Networks](https://arxiv.org/abs/1905.06886)||
|5.14.19|[Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848)||
|4.30.19|[Augmented Neural ODEs](https://arxiv.org/abs/1904.01681)||
|4.9.19|[Wasserstein Dependency Measure for Representation Learning](https://arxiv.org/abs/1903.11780)||
|4.2.19|[Leveraging Knowledge Bases in LSTMs for Improving Machine Reading](https://arxiv.org/abs/1902.09091)||
|3.26.19|[Meta Particle Flow for Sequential Bayesian Inference](https://arxiv.org/abs/1902.00640)||
|3.19.19|[A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms](https://arxiv.org/abs/1901.10912)||
|3.12.19|[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)||
|2.26.19|[Language Models are Unsupervised Multitask Learners](https://openai.com/blog/better-language-models/)|[openai](https://github.com/openai/gpt-2)|
|2.19.19|[Learning to Understand Goal Specifications by Modelling Reward](https://openreview.net/forum?id=H1xsSjC9Ym)||
|1.29.19|[GamePad: A Learning Environment for Theorem Proving](https://arxiv.org/abs/1806.00608)||
|1.15.19|[Matrix capsules with EM routing](https://openreview.net/forum?id=HJWLfGWRb&noteId=HJWLfGWRb)||
|12.4.18|[Optimizing Agent Behavior over Long Time Scales by Transporting Value](https://arxiv.org/abs/1810.06721)||
|11.27.18|[Embedding Logical Queries on Knowledge Graphs](https://arxiv.org/abs/1806.01445)|[williamleif](https://github.com/williamleif/graphqembed)|
|11.20.18|[Large-Scale Study of Curiosity-Driven Learning](https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf)|[openai](https://pathak22.github.io/large-scale-curiosity/)|
|11.13.18|[Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding](https://arxiv.org/abs/1809.03702)|[nke001](https://github.com/nke001/sparse_attentive_backtracking_release)|
|11.6.18|[Generalizing Hamiltonian Monte Carlo with Neural Networks](https://arxiv.org/abs/1711.09268)|[brain-research](https://github.com/brain-research/l2hmc)|
|10.23.18|[A Conceptual Introduction to Hamiltonian Monte Carlo](https://arxiv.org/abs/1701.02434)||
|10.16.18|[MaskGAN: Better Text Generation via Filling in the ...](https://arxiv.org/abs/1801.07736)||
|10.9.18|[Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096)||
|10.2.18|[Improving Variational Inference with Inverse Autoregressive Flow](https://arxiv.org/abs/1606.04934)||
|9.25.18|[Artificial Intelligence - The Revolution Hasn’t Happened Yet](https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7)||
|9.18.18|[Learning deep representations by mutual information estimation and maximization](https://arxiv.org/abs/1808.06670)||
|9.11.18|[The Variational Homoencoder: Learning to learn high capacity generative models from few examples](https://arxiv.org/abs/1807.08919)|[insperatum](https://github.com/insperatum/vhe)|
|9.4.18|[Towards Conceptual Compression](https://arxiv.org/abs/1604.08772)|[geosada](https://github.com/geosada/ConvDRAW)|
|8.28.18|[Vector-based navigation using grid-like representations in artificial agents](https://pdfs.semanticscholar.org/ae62/3c2364ce5eeaf16a7b11932908d1e413f179.pdf)|[deepmind](https://github.com/deepmind/grid-cells)|
||break in maintaining this file; filled on April 10, 2020||
| ------ | ------------- |------------- |
|8.21.18|[Universal Transformers](https://arxiv.org/abs/1807.03819)|[tensorflow](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py)|
|8.14.18|[Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)|[gautam1858](https://github.com/gautam1858/Neural-Arithmetic-Logic-Units)|
|8.7.18|[Neural Scene Representation and Rendering](https://deepmind.com/documents/211/Neural_Scene_Representation_and_Rendering_preprint.pdf)||
|7.31.18|[Measuring Abstract Reasoning in Neural Networks](https://arxiv.org/abs/1807.04225)||
|6.26.18|[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)|[openai](https://github.com/openai/finetune-transformer-lm)|
|6.19.18|[Associative Compression Networks for Representation Learning](https://arxiv.org/abs/1804.02476)||
|6.12.18|[On Characterizing the Capacity of Neural Networks using Algebraic Topology](https://arxiv.org/pdf/1802.04443.pdf)||
|6.5.18|[Causal Effect Inference with Deep Latent-Variable Models](https://arxiv.org/abs/1705.08821)|[AMLab](https://github.com/AMLab-Amsterdam/CEVAE)|
|5.29.18|[ML beyond Curve Fitting](http://www.inference.vc/untitled/)||
|5.22.18|[Synthesizing Programs for Images using Reinforced Adversarial Learning](https://arxiv.org/abs/1804.01118)||
|5.15.18|TensorFlow Overview|[r1.8](https://github.com/tensorflow/tensorflow/tree/r1.8)|
|5.8.18|[Compositional Attention Networks for Machine Reasoning](https://arxiv.org/abs/1803.03067)|[stanfordnlp](https://github.com/stanfordnlp/mac-network)|
|4.24.18|[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)||
|4.3.18|[How Developers Iterate on Machine Learning Workflows](https://arxiv.org/abs/1803.10311)||
|3.27.18|[Faster R-CNN: Towards Real-Time Object,Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)||
|3.20.18|[Attention Is All You Need](https://arxiv.org/abs/1706.03762)|[tensor2tensor](https://github.com/tensorflow/tensor2tensor)|
|3.6.18|[Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/abs/1801.10198)|[wikisum](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum), per [this gist](https://gist.github.com/peterjliu/f0dc9152a630520dc604c783db963aa7)|
|2.27.18|[AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks](https://arxiv.org/abs/1711.10485)|[StackGAN-v2](https://github.com/hanzhanggit/StackGAN-v2)|
|2.20.18|[Information Dropout](https://arxiv.org/pdf/1611.01353.pdf)|[InformationDropout](https://github.com/coventry/InformationDropout/blob/master/information_dropout.py), [official implementation](https://github.com/ucla-vision/information-dropout)|
|2.13.18|[Nested LSTMs](https://arxiv.org/abs/1801.10308)|[Nested-LSTM](https://github.com/titu1994/Nested-LSTM)|
|2.6.18|[Deep vs. Shallow Networks: An Approximation Theory Perspective](https://arxiv.org/abs/1608.03287)|
|1.30.18|[The Case for Learned Index Structures](https://arxiv.org/abs/1712.01208)|
|1.23.18|[Visualizing The Loss Landscape Of Neural Nets](https://arxiv.org/pdf/1712.09913.pdf)|
|1.16.18|[Go for a Walk and Arrive at the Answer](https://arxiv.org/abs/1711.05851), [RelNet: End-to-End Modeling of Entities & Relations](https://arxiv.org/abs/1706.07179)|
|1.9.18|[Intro to Coq](http://adam.chlipala.net/cpdt/) |
|12.12.17|[Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks](https://arxiv.org/abs/1607.01426) | [(ChainsofReasoning)](https://rajarshd.github.io/ChainsofReasoning/)
|12.5.17|[Stochastic Neural Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1704.03012)|[snn4hrl](https://github.com/florensacc/snn4hrl)|
|11.28.17|[Emergent Complexity via Multi-Agent Competition](https://arxiv.org/abs/1710.03748) [(blog post)](https://blog.openai.com/competitive-self-play/?)|[multiagent-competition](https://github.com/openai/multiagent-competition)|
|11.14.17|[Mastering the game of Go without human knowledge](https://deepmind.com/documents/119/agz_unformatted_nature.pdf) ||
|11.7.17|[Meta-Learning with Memory-Augmented Neural Networks](http://proceedings.mlr.press/v48/santoro16.html)|[ntm-meta-learning](https://github.com/ywatanabex/ntm-meta-learning)|
|10.24.17|[Poincaré Embeddings for Learning Hierarchical Representations](https://arxiv.org/abs/1705.08039)|[poincare_embeddings](https://github.com/nishnik/poincare_embeddings)|
|10.17.17|[What does Attention in Neural Machine Translation Pay Attention to?](https://arxiv.org/abs/1710.03348v1)||
|10.10.17|[Zero-Shot Learning Through Cross-Modal Transfer](http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer.pdf)|[zslearning](https://github.com/mganjoo/zslearning)|
|9.26.17|[Variational Boosting: Iteratively Refining Posterior Approximations](https://arxiv.org/abs/1611.06585)|[vboost](https://github.com/andymiller/vboost)|
|9.19.17|[Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)|[cbfinn](https://github.com/cbfinn)|
|9.12.17|[Neuroscience-inspired AI](http://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3)||
|9.5.17|[Recurrent Dropout Without Memory Loss](https://arxiv.org/abs/1603.05118)|[rnn_cell_mulint_modern.py](https://github.com/NickShahML/tensorflow_with_latest_papers/blob/master/rnn_cell_mulint_modern.py#L141)|
|8.29.17|[Deep Transfer Learning with Joint Adaptation Networks](https://arxiv.org/abs/1605.06636)|jmmd.{[cpp](https://github.com/thuml/transfer-caffe/blob/41455ac37f11c18fb19509c93b381bcd51ded68e/src/caffe/layers/jmmd_layer.cpp),[hpp](https://github.com/thuml/transfer-caffe/blob/41455ac37f11c18fb19509c93b381bcd51ded68e/src/caffe/layers/jmmd_layer.cpp)}|
|8.22.17|[Designing Neural Network Architectures using Reinforcement Learning](https://arxiv.org/abs/1611.02167)|[metaqnn](https://github.com/bowenbaker/metaqnn)|
|8.15.17|[Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences](https://arxiv.org/abs/1610.09513)|[plstm](https://github.com/dannyneil/public_plstm)|
|8.8.17|[Hyper Networks](https://arxiv.org/abs/1609.09106)|[otoro blog](http://blog.otoro.net/2016/09/28/hyper-networks/)|
|8.1.17|[Full-Capacity Unitary Recurrent Neural Networks](https://arxiv.org/pdf/1611.00035.pdf)|[complex_RNN](https://github.com/amarshah/complex_RNN), [urnn](https://github.com/stwisdom/urnn)|
|7.25.17|[Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343) & [follow-up](https://arxiv.org/abs/1703.00522)|[dni.pytorch](https://github.com/andrewliao11/dni.pytorch)|
|7.18.17|[A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427)|[relation-network](https://github.com/Alan-Lee123/relation-network)|
|7.11.17|[Speaker diarization using deep neural network embeddings](http://www.danielpovey.com/files/2017_icassp_diarization_embeddings.pdf)||
|6.20.17|[Neural Episodic Control](https://arxiv.org/abs/1703.01988)|[PFCM](https://github.com/PFCM/neural-episodic-control)|
|6.13.17|[Lie-Access Neural Turing Machines](https://arxiv.org/abs/1611.02854)|[harvardnlp](https://github.com/harvardnlp/lie-access-memory)|
|6.6.17|[Artistic style transfer for videos](https://arxiv.org/abs/1604.08610)|[artistic video](https://github.com/manuelruder/artistic-videos)|
|5.30.17|[High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)|[modular_rl](https://github.com/joschu/modular_rl)|
|5.23.17|[Emergence of Grounded Compositional Language in Multi-Agent Populations](https://arxiv.org/abs/1703.04908)||
|5.16.17|[Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)|[modular_rl](https://github.com/joschu/modular_rl)|
|5.9.17|[Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)|[code](https://github.com/igul222/improved_wgan_training)|
|5.4.17|[Using Fast Weights to Attend to the Recent Past](https://arxiv.org/abs/1610.06258)||
|4.25.17|[Strategic Attentive Writer for Learning Macro-Actions](https://arxiv.org/abs/1606.04695)||
|4.18.17|[Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/abs/1703.03906)||
|4.4.17|[End to End Learning for Self-Driving Cars](https://arxiv.org/abs/1604.07316)||
|3.28.17|[Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning](https://arxiv.org/abs/1509.08731)||
|3.21.17|[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)||
|3.7.17|[Neural Programmer Interpreters](https://arxiv.org/abs/1511.06279)||
|2.14.17|[Wasserstein GAN](https://arxiv.org/abs/1701.07875)||
|2.7.17|[Towards Principled Methods for Training GANs](https://arxiv.org/abs/1701.04862)||
|1.31.17|[Mastering the Game of Go with Deep Networks](http://airesearch.com/wp-content/uploads/2016/01/deepmind-mastering-go.pdf)||
|1.24.17|[Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530)||
|1.17.17|[Neural Semantic Encoders](https://arxiv.org/abs/1607.04315)||
|12.21.16|[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/abs/1612.03242/)||
|12.14.16|[Key-Value Memory Networks for Directly Reading Documents](https://arxiv.org/abs/1606.03126)||
| 12.7.16  | [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657)  |
