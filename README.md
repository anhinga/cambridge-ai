# SIPB Deep Learning Group
The schedule of readings for the SIPB/Cambridge AI Deep Learning Group If you have any papers you'd like to discuss, please either make a pull request, or send an email to the group and we'll add it. Papers with implementations available are strongly preferred.


## Suggested Papers:

* [Taskonomy: Disentangling Task Transfer Learning](https://arxiv.org/pdf/1804.08328.pdf)


## Schedule:


| Date  | Paper | Implementation |
| ------------- | ------------- |------------- |
|6.25.20|[DreamCoder: Building interpretable hierarchical knowledge representations with wake-sleep Bayesian program learning](https://web.mit.edu/ellisk/www/documents/dreamcoder_with_supplement.pdf)|[ellisk42/ec](https://github.com/ellisk42/ec)|
|6.18.20|[SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver](https://arxiv.org/abs/1905.12149)|[locuslab/SATNet](https://github.com/locuslab/SATNet)|
|6.4.20|[Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)||
|5.28.20|[Complexity control by gradient descent in deep networks](https://www.nature.com/articles/s41467-020-14663-9)||
|5.21.20|[What Can Learned Intrinsic Rewards Capture?](https://arxiv.org/abs/1912.05500)||
|5.14.20|[COMET: Commonsense Transformers for Automatic Knowledge Graph Construction](https://arxiv.org/abs/1906.05317)||
|5.7.20|[Write, Execute, Assess: Program Synthesis With a REPL](http://papers.nips.cc/paper/9116-write-execute-assess-program-synthesis-with-a-repl.pdf)|[flxsosa/ProgramSearch](https://github.com/flxsosa/ProgramSearch)|
|4.23.20|[Graph Representations for Higher-Order Logic and Theorem Proving](https://arxiv.org/abs/1905.10006)||
|4.16.20|[Mathematical Reasoning in Latent Space](https://openreview.net/pdf?id=Ske31kBtPr)||
|4.9.20|[MEMO: A Deep Network for Flexible Combination of Episodic Memories](https://arxiv.org/abs/2001.10913)||
|4.2.20|[Creating High Resolution Images with a Latent Adversarial Generator](https://arxiv.org/abs/2003.02365)||
|3.26.20|[Invertible Residual Networks](https://arxiv.org/abs/1811.00995)||
|3.5.20|[Value-driven Hindsight Modelling](https://arxiv.org/abs/2002.08329)||
|2.27.20|[Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958)||
|2.13.20|[Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365)||
|2.6.20|[Automated curricula through setter-solver interactions](https://arxiv.org/abs/1909.12892)||
|1.30.20|[Protein structure prediction ...](https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25834)|[deepmind](https://github.com/deepmind/deepmind-research/tree/master/alphafold_casp13)|
|1.23.20|[Putting An End to End-to-End: Gradient-Isolated Learning of Representations](https://arxiv.org/abs/1905.11786)||
|1.16.20|[Normalizing Flows: An Introduction and Review of Current Methods](https://arxiv.org/abs/1908.09257)||
|12.19.19|[Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265)||
|12.5.19|[On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)||
|11.21.19|[Understanding the Neural Tangent Kernel](https://rajatvd.github.io/NTK/)|[rajatvd](https://github.com/rajatvd/NTK)|
|11.14.19|[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)||
|11.7.19|[Learning to Predict Without Looking Ahead: World Models Without Forward Prediction](https://arxiv.org/abs/1910.13038)||
|10.31.19|[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)||
|10.24.19|[N-BEATS: Neural basis expansion analysis for interpretable time series forecasting](https://arxiv.org/abs/1905.10437)||
|10.17.19|[Unsupervised Doodling and Painting with Improved SPIRAL](https://arxiv.org/abs/1910.01007)||
|10.10.19|[Adversarial Robustness as a Prior for Learned Representations](https://arxiv.org/abs/1906.00945)|[MadryLab](https://github.com/MadryLab/robust_representations)|
|10.3.19|[Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks](https://arxiv.org/abs/1805.12076)||
|9.26.19|[Image Transformer](https://arxiv.org/abs/1802.05751)||
|9.19.19|[Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446)||
|9.12.19|[Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)||
|9.5.19|[Neural Text Generation with Unlikelihood Training](https://arxiv.org/abs/1908.04319)||
|8.29.19|[Learning Representations by Maximizing Mutual Information Across Views](https://arxiv.org/abs/1906.00910)||
|break|switch from Tuesdays to Thursdays after the break||
|6.11.19|[BERT Rediscovers the Classical NLP Pipeline](https://arxiv.org/abs/1905.05950)||
|6.4.19|[Semantic Visual Localization](https://arxiv.org/abs/1712.05773)||
|5.28.19|[AlgoNet: C^∞ Smooth Algorithmic Neural Networks](https://arxiv.org/abs/1905.06886)||
|5.14.19|[Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848)||
|4.30.19|[Augmented Neural ODEs](https://arxiv.org/abs/1904.01681)||
|4.9.19|[Wasserstein Dependency Measure for Representation Learning](https://arxiv.org/abs/1903.11780)||
|4.2.19|[Leveraging Knowledge Bases in LSTMs for Improving Machine Reading](https://arxiv.org/abs/1902.09091)||
|3.26.19|[Meta Particle Flow for Sequential Bayesian Inference](https://arxiv.org/abs/1902.00640)||
|3.19.19|[A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms](https://arxiv.org/abs/1901.10912)||
|3.12.19|[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)||
|2.26.19|[Language Models are Unsupervised Multitask Learners](https://openai.com/blog/better-language-models/)|[openai](https://github.com/openai/gpt-2)|
|2.19.19|[Learning to Understand Goal Specifications by Modelling Reward](https://openreview.net/forum?id=H1xsSjC9Ym)||
|1.29.19|[GamePad: A Learning Environment for Theorem Proving](https://arxiv.org/abs/1806.00608)||
|1.15.19|[Matrix capsules with EM routing](https://openreview.net/forum?id=HJWLfGWRb&noteId=HJWLfGWRb)||
|12.4.18|[Optimizing Agent Behavior over Long Time Scales by Transporting Value](https://arxiv.org/abs/1810.06721)||
|11.27.18|[Embedding Logical Queries on Knowledge Graphs](https://arxiv.org/abs/1806.01445)|[williamleif](https://github.com/williamleif/graphqembed)|
|11.20.18|[Large-Scale Study of Curiosity-Driven Learning](https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf)|[openai](https://pathak22.github.io/large-scale-curiosity/)|
|11.13.18|[Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding](https://arxiv.org/abs/1809.03702)|[nke001](https://github.com/nke001/sparse_attentive_backtracking_release)|
|11.6.18|[Generalizing Hamiltonian Monte Carlo with Neural Networks](https://arxiv.org/abs/1711.09268)|[brain-research](https://github.com/brain-research/l2hmc)|
|10.23.18|[A Conceptual Introduction to Hamiltonian Monte Carlo](https://arxiv.org/abs/1701.02434)||
|10.16.18|[MaskGAN: Better Text Generation via Filling in the ...](https://arxiv.org/abs/1801.07736)||
|10.9.18|[Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096)||
|10.2.18|[Improving Variational Inference with Inverse Autoregressive Flow](https://arxiv.org/abs/1606.04934)||
|9.25.18|[Artificial Intelligence - The Revolution Hasn’t Happened Yet](https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7)||
|9.18.18|[Learning deep representations by mutual information estimation and maximization](https://arxiv.org/abs/1808.06670)||
|9.11.18|[The Variational Homoencoder: Learning to learn high capacity generative models from few examples](https://arxiv.org/abs/1807.08919)|[insperatum](https://github.com/insperatum/vhe)|
|9.4.18|[Towards Conceptual Compression](https://arxiv.org/abs/1604.08772)|[geosada](https://github.com/geosada/ConvDRAW)|
|8.28.18|[Vector-based navigation using grid-like representations in artificial agents](https://pdfs.semanticscholar.org/ae62/3c2364ce5eeaf16a7b11932908d1e413f179.pdf)|[deepmind](https://github.com/deepmind/grid-cells)|
||break in maintaining this file; filled on April 10, 2020||
| ------ | ------------- |------------- |
|8.21.18|[Universal Transformers](https://arxiv.org/abs/1807.03819)|[tensorflow](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py)|
|8.14.18|[Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)|[gautam1858](https://github.com/gautam1858/Neural-Arithmetic-Logic-Units)|
|8.7.18|[Neural Scene Representation and Rendering](https://deepmind.com/documents/211/Neural_Scene_Representation_and_Rendering_preprint.pdf)||
|7.31.18|[Measuring Abstract Reasoning in Neural Networks](https://arxiv.org/abs/1807.04225)||
|6.26.18|[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)|[openai](https://github.com/openai/finetune-transformer-lm)|
|6.19.18|[Associative Compression Networks for Representation Learning](https://arxiv.org/abs/1804.02476)||
|6.12.18|[On Characterizing the Capacity of Neural Networks using Algebraic Topology](https://arxiv.org/pdf/1802.04443.pdf)||
|6.5.18|[Causal Effect Inference with Deep Latent-Variable Models](https://arxiv.org/abs/1705.08821)|[AMLab](https://github.com/AMLab-Amsterdam/CEVAE)|
|5.29.18|[ML beyond Curve Fitting](http://www.inference.vc/untitled/)||
|5.22.18|[Synthesizing Programs for Images using Reinforced Adversarial Learning](https://arxiv.org/abs/1804.01118)||
|5.15.18|TensorFlow Overview|[r1.8](https://github.com/tensorflow/tensorflow/tree/r1.8)|
|5.8.18|[Compositional Attention Networks for Machine Reasoning](https://arxiv.org/abs/1803.03067)|[stanfordnlp](https://github.com/stanfordnlp/mac-network)|
|4.24.18|[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)||
|4.3.18|[How Developers Iterate on Machine Learning Workflows](https://arxiv.org/abs/1803.10311)||
|3.27.18|[Faster R-CNN: Towards Real-Time Object,Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)||
|3.20.18|[Attention Is All You Need](https://arxiv.org/abs/1706.03762)|[tensor2tensor](https://github.com/tensorflow/tensor2tensor)|
|3.6.18|[Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/abs/1801.10198)|[wikisum](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum), per [this gist](https://gist.github.com/peterjliu/f0dc9152a630520dc604c783db963aa7)|
|2.27.18|[AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks](https://arxiv.org/abs/1711.10485)|[StackGAN-v2](https://github.com/hanzhanggit/StackGAN-v2)|
|2.20.18|[Information Dropout](https://arxiv.org/pdf/1611.01353.pdf)|[InformationDropout](https://github.com/coventry/InformationDropout/blob/master/information_dropout.py), [official implementation](https://github.com/ucla-vision/information-dropout)|
|2.13.18|[Nested LSTMs](https://arxiv.org/abs/1801.10308)|[Nested-LSTM](https://github.com/titu1994/Nested-LSTM)|
|2.6.18|[Deep vs. Shallow Networks: An Approximation Theory Perspective](https://arxiv.org/abs/1608.03287)|
|1.30.18|[The Case for Learned Index Structures](https://arxiv.org/abs/1712.01208)|
|1.23.18|[Visualizing The Loss Landscape Of Neural Nets](https://arxiv.org/pdf/1712.09913.pdf)|
|1.16.18|[Go for a Walk and Arrive at the Answer](https://arxiv.org/abs/1711.05851), [RelNet: End-to-End Modeling of Entities & Relations](https://arxiv.org/abs/1706.07179)|
|1.9.18|[Intro to Coq](http://adam.chlipala.net/cpdt/) |
|12.12.17|[Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks](https://arxiv.org/abs/1607.01426) | [(ChainsofReasoning)](https://rajarshd.github.io/ChainsofReasoning/)
|12.5.17|[Stochastic Neural Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1704.03012)|[snn4hrl](https://github.com/florensacc/snn4hrl)|
|11.28.17|[Emergent Complexity via Multi-Agent Competition](https://arxiv.org/abs/1710.03748) [(blog post)](https://blog.openai.com/competitive-self-play/?)|[multiagent-competition](https://github.com/openai/multiagent-competition)|
|11.14.17|[Mastering the game of Go without human knowledge](https://deepmind.com/documents/119/agz_unformatted_nature.pdf) ||
|11.7.17|[Meta-Learning with Memory-Augmented Neural Networks](http://proceedings.mlr.press/v48/santoro16.html)|[ntm-meta-learning](https://github.com/ywatanabex/ntm-meta-learning)|
|10.24.17|[Poincaré Embeddings for Learning Hierarchical Representations](https://arxiv.org/abs/1705.08039)|[poincare_embeddings](https://github.com/nishnik/poincare_embeddings)|
|10.17.17|[What does Attention in Neural Machine Translation Pay Attention to?](https://arxiv.org/abs/1710.03348v1)||
|10.10.17|[Zero-Shot Learning Through Cross-Modal Transfer](http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer.pdf)|[zslearning](https://github.com/mganjoo/zslearning)|
|9.26.17|[Variational Boosting: Iteratively Refining Posterior Approximations](https://arxiv.org/abs/1611.06585)|[vboost](https://github.com/andymiller/vboost)|
|9.19.17|[Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)|[cbfinn](https://github.com/cbfinn)|
|9.12.17|[Neuroscience-inspired AI](http://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3)||
|9.5.17|[Recurrent Dropout Without Memory Loss](https://arxiv.org/abs/1603.05118)|[rnn_cell_mulint_modern.py](https://github.com/NickShahML/tensorflow_with_latest_papers/blob/master/rnn_cell_mulint_modern.py#L141)|
|8.29.17|[Deep Transfer Learning with Joint Adaptation Networks](https://arxiv.org/abs/1605.06636)|jmmd.{[cpp](https://github.com/thuml/transfer-caffe/blob/41455ac37f11c18fb19509c93b381bcd51ded68e/src/caffe/layers/jmmd_layer.cpp),[hpp](https://github.com/thuml/transfer-caffe/blob/41455ac37f11c18fb19509c93b381bcd51ded68e/src/caffe/layers/jmmd_layer.cpp)}|
|8.22.17|[Designing Neural Network Architectures using Reinforcement Learning](https://arxiv.org/abs/1611.02167)|[metaqnn](https://github.com/bowenbaker/metaqnn)|
|8.15.17|[Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences](https://arxiv.org/abs/1610.09513)|[plstm](https://github.com/dannyneil/public_plstm)|
|8.8.17|[Hyper Networks](https://arxiv.org/abs/1609.09106)|[otoro blog](http://blog.otoro.net/2016/09/28/hyper-networks/)|
|8.1.17|[Full-Capacity Unitary Recurrent Neural Networks](https://arxiv.org/pdf/1611.00035.pdf)|[complex_RNN](https://github.com/amarshah/complex_RNN), [urnn](https://github.com/stwisdom/urnn)|
|7.25.17|[Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343) & [follow-up](https://arxiv.org/abs/1703.00522)|[dni.pytorch](https://github.com/andrewliao11/dni.pytorch)|
|7.18.17|[A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427)|[relation-network](https://github.com/Alan-Lee123/relation-network)|
|7.11.17|[Speaker diarization using deep neural network embeddings](http://www.danielpovey.com/files/2017_icassp_diarization_embeddings.pdf)||
|6.20.17|[Neural Episodic Control](https://arxiv.org/abs/1703.01988)|[PFCM](https://github.com/PFCM/neural-episodic-control)|
|6.13.17|[Lie-Access Neural Turing Machines](https://arxiv.org/abs/1611.02854)|[harvardnlp](https://github.com/harvardnlp/lie-access-memory)|
|6.6.17|[Artistic style transfer for videos](https://arxiv.org/abs/1604.08610)|[artistic video](https://github.com/manuelruder/artistic-videos)|
|5.30.17|[High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)|[modular_rl](https://github.com/joschu/modular_rl)|
|5.23.17|[Emergence of Grounded Compositional Language in Multi-Agent Populations](https://arxiv.org/abs/1703.04908)||
|5.16.17|[Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)|[modular_rl](https://github.com/joschu/modular_rl)|
|5.9.17|[Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)|[code](https://github.com/igul222/improved_wgan_training)|
|5.4.17|[Using Fast Weights to Attend to the Recent Past](https://arxiv.org/abs/1610.06258)||
|4.25.17|[Strategic Attentive Writer for Learning Macro-Actions](https://arxiv.org/abs/1606.04695)||
|4.18.17|[Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/abs/1703.03906)||
|4.4.17|[End to End Learning for Self-Driving Cars](https://arxiv.org/abs/1604.07316)||
|3.28.17|[Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning](https://arxiv.org/abs/1509.08731)||
|3.21.17|[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)||
|3.7.17|[Neural Programmer Interpreters](https://arxiv.org/abs/1511.06279)||
|2.14.17|[Wasserstein GAN](https://arxiv.org/abs/1701.07875)||
|2.7.17|[Towards Principled Methods for Training GANs](https://arxiv.org/abs/1701.04862)||
|1.31.17|[Mastering the Game of Go with Deep Networks](http://airesearch.com/wp-content/uploads/2016/01/deepmind-mastering-go.pdf)||
|1.24.17|[Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530)||
|1.17.17|[Neural Semantic Encoders](https://arxiv.org/abs/1607.04315)||
|12.21.16|[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/abs/1612.03242/)||
|12.14.16|[Key-Value Memory Networks for Directly Reading Documents](https://arxiv.org/abs/1606.03126)||
| 12.7.16  | [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657)  |
